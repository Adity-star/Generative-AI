{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U  langchain_community langchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "id": "KrofPdUMpRHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"NVIDIA_API_KEY\")"
      ],
      "metadata": {
        "id": "t5Ppk4jLpE6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.1-70b-instruct\")"
      ],
      "metadata": {
        "id": "N1rWweRfpHPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic Few-Shot"
      ],
      "metadata": {
        "id": "6rhG_Nl_pNrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_sentiment_classification(input_text):\n",
        "    few_shot_prompt = PromptTemplate(\n",
        "        input_variables=[\"input_text\"],\n",
        "        template=\"\"\"\n",
        "        Classify the sentiment as Positive, Negative, or Neutral.\n",
        "\n",
        "        Examples:\n",
        "        Text: I love this product! It's amazing.\n",
        "        Sentiment: Positive\n",
        "\n",
        "        Text: This movie was terrible. I hated it.\n",
        "        Sentiment: Negative\n",
        "\n",
        "        Text: The weather today is okay.\n",
        "        Sentiment: Neutral\n",
        "\n",
        "        Now, classify the following:\n",
        "        Text: {input_text}\n",
        "        Sentiment:\n",
        "        \"\"\"\n",
        "    )\n",
        "    chain = few_shot_prompt | llm\n",
        "    result = chain.invoke(input_text).content\n",
        "\n",
        "    # Clean up the result\n",
        "    result = result.strip()\n",
        "    # Extract only the sentiment label\n",
        "    if ':' in result:\n",
        "        result = result.split(':')[1].strip()\n",
        "\n",
        "    return result  # This will now return just \"Positive\", \"Negative\", or \"Neutral\"\n",
        "\n",
        "test_text = \"I can't believe how great this new restaurant is!\"\n",
        "result = few_shot_sentiment_classification(test_text)\n",
        "print(f\"Input: {test_text}\")\n",
        "print(f\"Predicted Sentiment: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHIUfR7xpoBB",
        "outputId": "7652342f-48c6-46cb-cf8b-c2ec765cdf98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I can't believe how great this new restaurant is!\n",
            "Predicted Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Advanced Few-shot"
      ],
      "metadata": {
        "id": "0vB-FykYpn9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_task_few_shot(input_text, task):\n",
        "    few_shot_prompt = PromptTemplate(\n",
        "        input_variables=[\"input_text\", \"task\"],\n",
        "        template=\"\"\"\n",
        "        Perform the specified task on the given text.\n",
        "\n",
        "        Examples:\n",
        "        Text: I love this product! It's amazing.\n",
        "        Task: sentiment\n",
        "        Result: Positive\n",
        "\n",
        "        Text: Bonjour, comment allez-vous?\n",
        "        Task: language\n",
        "        Result: French\n",
        "\n",
        "        Now, perform the following task:\n",
        "        Text: {input_text}\n",
        "        Task: {task}\n",
        "        Result:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = few_shot_prompt | llm\n",
        "    return chain.invoke({\"input_text\": input_text, \"task\": task}).content\n",
        "\n",
        "print(multi_task_few_shot(\"I can't believe how great this is!\", \"sentiment\"))\n",
        "print(multi_task_few_shot(\"Guten Tag, wie geht es Ihnen?\", \"language\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3eA2Y8Upn7G",
        "outputId": "b30a6b72-888a-4ee1-c350-29208793df83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n",
            "German\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. In-Context Learning"
      ],
      "metadata": {
        "id": "5Mx14S0cpn4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def in_context_learning(task_description, examples, input_text):\n",
        "    example_text = \"\".join([f\"Input: {e['input']}\\nOutput: {e['output']}\\n\\n\" for e in examples])\n",
        "\n",
        "    in_context_prompt = PromptTemplate(\n",
        "        input_variables=[\"task_description\", \"examples\", \"input_text\"],\n",
        "        template=\"\"\"\n",
        "        Task: {task_description}\n",
        "\n",
        "        Examples:\n",
        "        {examples}\n",
        "\n",
        "        Now, perform the task on the following input:\n",
        "        Input: {input_text}\n",
        "        Output:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = in_context_prompt | llm\n",
        "    return chain.invoke({\"task_description\": task_description, \"examples\": example_text, \"input_text\": input_text}).content\n",
        "\n",
        "task_desc = \"Convert the given text to pig latin.\"\n",
        "examples = [\n",
        "    {\"input\": \"hello\", \"output\": \"ellohay\"},\n",
        "    {\"input\": \"apple\", \"output\": \"appleay\"}\n",
        "]\n",
        "test_input = \"python\"\n",
        "\n",
        "result = in_context_learning(task_desc, examples, test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Output: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apJlbA9Dpn18",
        "outputId": "d6c743b1-c30c-4acf-ae13-eeedabdc1498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: python\n",
            "Output: ythonpay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_func, test_cases):\n",
        "    '''\n",
        "    Evaluate the model on a set of test cases.\n",
        "\n",
        "    Args:\n",
        "    model_func: The function that makes predictions.\n",
        "    test_cases: A list of dictionaries, where each dictionary contains an \"input\" text and a \"label\" for the input.\n",
        "\n",
        "    Returns:\n",
        "    The accuracy of the model on the test cases.\n",
        "    '''\n",
        "    correct = 0\n",
        "    total = len(test_cases)\n",
        "\n",
        "    for case in test_cases:\n",
        "        input_text = case['input']\n",
        "        true_label = case['label']\n",
        "        prediction = model_func(input_text).strip()\n",
        "\n",
        "        is_correct = prediction.lower() == true_label.lower()\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        print(f\"Input: {input_text}\")\n",
        "        print(f\"Predicted: {prediction}\")\n",
        "        print(f\"Actual: {true_label}\")\n",
        "        print(f\"Correct: {is_correct}\\n\")\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "test_cases = [\n",
        "    {\"input\": \"This product exceeded my expectations!\", \"label\": \"Positive\"},\n",
        "    {\"input\": \"I'm utterly disappointed with the service.\", \"label\": \"Negative\"},\n",
        "    {\"input\": \"The temperature today is 72 degrees.\", \"label\": \"Neutral\"}\n",
        "]\n",
        "\n",
        "accuracy = evaluate_model(few_shot_sentiment_classification, test_cases)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VUcpu2JqRYM",
        "outputId": "180db434-2930-486c-d1fc-f872cdf3f060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: This product exceeded my expectations!\n",
            "Predicted: Positive\n",
            "Actual: Positive\n",
            "Correct: True\n",
            "\n",
            "Input: I'm utterly disappointed with the service.\n",
            "Predicted: Negative\n",
            "Actual: Negative\n",
            "Correct: True\n",
            "\n",
            "Input: The temperature today is 72 degrees.\n",
            "Predicted: Neutral\n",
            "\n",
            "The text simply states a fact about the temperature without expressing any emotion or opinion.\n",
            "Actual: Neutral\n",
            "Correct: False\n",
            "\n",
            "Model Accuracy: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfUUowHiqVAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}