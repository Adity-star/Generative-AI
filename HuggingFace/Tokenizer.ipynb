{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9EWOgc4ynnE",
        "outputId": "e3ce1db4-016f-4975-bea4-8e7e2cc4dd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgHKuFxgyC0R"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('wikitext', name='wikitext-2-raw-v1', split='train')\n",
        "\n",
        "def get_training_corpus():\n",
        " for i in range(0, len(dataset), 1000):\n",
        "    yield dataset[i : i + 1000]['text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wikitext-2.txt', 'w', encoding='utf-8') as f:\n",
        "  for i in range(len(dataset)):\n",
        "    f.write(dataset[i]['text'] + '\\n')"
      ],
      "metadata": {
        "id": "r-DrhjNGysG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordPiece"
      ],
      "metadata": {
        "id": "3zz3bHiizKaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import (decoders,\n",
        "                       models,\n",
        "                       normalizers,\n",
        "                       pre_tokenizers,\n",
        "                       processors,\n",
        "                        trainers,\n",
        "                        Tokenizer)\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))"
      ],
      "metadata": {
        "id": "o2bP9hfKzXrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ],
      "metadata": {
        "id": "EvZWx0A0zXoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ],
      "metadata": {
        "id": "mUw27JlFzXls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQhQ0XL5zXjB",
        "outputId": "e5031357-172a-4981-a16f-e0945cf86168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are u?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ],
      "metadata": {
        "id": "_cCAfGWNzXgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ],
      "metadata": {
        "id": "F_djnpo60On5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq2GkBGN0QQ3",
        "outputId": "c76bdb83-7ea7-4fcd-9f49-b43e3a165e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'\", (3, 4)),\n",
              " ('s', (4, 5)),\n",
              " ('test', (6, 10)),\n",
              " ('my', (11, 13)),\n",
              " ('pre', (14, 17)),\n",
              " ('-', (17, 18)),\n",
              " ('tokenizer', (18, 27)),\n",
              " ('.', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BgnvJ6I0Rje",
        "outputId": "bd83575a-5501-4394-f2d6-da3400a70bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Let's\", (0, 5)),\n",
              " ('test', (6, 10)),\n",
              " ('my', (11, 13)),\n",
              " ('pre-tokenizer.', (14, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence(\n",
        "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
        ")\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19hXOt720TtR",
        "outputId": "abb78b90-7e75-4472-8c59-e47ba9c751de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'\", (3, 4)),\n",
              " ('s', (4, 5)),\n",
              " ('test', (6, 10)),\n",
              " ('my', (11, 13)),\n",
              " ('pre', (14, 17)),\n",
              " ('-', (17, 18)),\n",
              " ('tokenizer', (18, 27)),\n",
              " ('.', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ],
      "metadata": {
        "id": "mL8f-pY-0VYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "XZemmqUU0W4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "yJAVnzrP0YIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBdPhdiU0m6g",
        "outputId": "06291e63-33bc-4f98-8cd9-46faa11671b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2tcZBB20oIe",
        "outputId": "5a87d9a2-d887-4e12-9d58-a81d32afc215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")"
      ],
      "metadata": {
        "id": "IcVYTE7o0qDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUXjHRPN0rp1",
        "outputId": "6a686c70-99dd-4308-98c1-0cb97c695500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gBV72s20tDi",
        "outputId": "07eedf3f-80cf-410a-bb83-aae1b293709d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '.', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ],
      "metadata": {
        "id": "keVE7dnL0uvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoding.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OxLdS5Yh0wPO",
        "outputId": "2d41aa79-5f5f-46b6-bbb4-38912f0f82ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"let ' s test this tokenizer... on a pair of sentences.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "YWB_AV8e0xi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ],
      "metadata": {
        "id": "7fhxEliJ0zHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ],
      "metadata": {
        "id": "COvLTs9Q1F0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE"
      ],
      "metadata": {
        "id": "cI9qd1_81HsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ],
      "metadata": {
        "id": "8bRuQpu-1LVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ],
      "metadata": {
        "id": "4dj1SzSe1LRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieXTTRYV1LO2",
        "outputId": "4f2d5e2f-b228-483f-fb92-b574106aa20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'s\", (3, 5)),\n",
              " ('Ġtest', (5, 10)),\n",
              " ('Ġpre', (10, 14)),\n",
              " ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)),\n",
              " ('!', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "rAHbKDlZ1LMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model = models.BPE()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "_SWKY0D91Q09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6Ky1gPi1SAT",
        "outputId": "0bc9a18f-0a9f-40ee-e564-d3d854ad50f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
      ],
      "metadata": {
        "id": "GuXAh6TA1TOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7Ewq0kUu1UZt",
        "outputId": "fae94c73-43fc-4c4f-d29a-e8cef40c0b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()"
      ],
      "metadata": {
        "id": "wsZDOD4i1WFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoding.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aeAKeIy71XVC",
        "outputId": "7086ac7d-812f-4771-dac9-348048837fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's test this tokenizer.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        ")"
      ],
      "metadata": {
        "id": "K6O3FQ_41YbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        ")"
      ],
      "metadata": {
        "id": "acturIg61ZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ],
      "metadata": {
        "id": "j4jeTYPD1a_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram"
      ],
      "metadata": {
        "id": "aR3SHfTs1cbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ],
      "metadata": {
        "id": "4dUAhA1P1e10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.Replace(\"``\", '\"'),\n",
        "        normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),\n",
        "        normalizers.StripAccents(),\n",
        "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "WymroCui1eyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ],
      "metadata": {
        "id": "04olEaWt1evx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dPGnf6i1eto",
        "outputId": "e0840bc2-a061-4b68-bc54-e4fa18ee101e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"▁Let's\", (0, 5)),\n",
              " ('▁test', (5, 10)),\n",
              " ('▁the', (10, 14)),\n",
              " ('▁pre-tokenizer!', (14, 29))]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
        ")\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "_4C0TYHM1jq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model = models.Unigram()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "Lqrilky51k5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdadAQjG1mQo",
        "outputId": "cdf9798d-e9a5-411e-e175-9e0078dae033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
        "print(cls_token_id, sep_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7A7F441nZ3",
        "outputId": "844d7002-748d-4968-f99b-b277f479a58d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")"
      ],
      "metadata": {
        "id": "BxwWgRcS1ouT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ],
      "metadata": {
        "id": "1wQjCEfX1qJt",
        "outputId": "88c6660c-9293-422c-bb77-8bb51b17fe14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ],
      "metadata": {
        "id": "q7XqMilI1rf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    cls_token=\"<cls>\",\n",
        "    sep_token=\"<sep>\",\n",
        "    mask_token=\"<mask>\",\n",
        "    padding_side=\"left\",\n",
        ")"
      ],
      "metadata": {
        "id": "u42GIwZn1s4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
      ],
      "metadata": {
        "id": "wTresmIn1uis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoJd3gwc1vp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}